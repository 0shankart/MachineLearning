{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {
        "id": "FtCVoFclN7FP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets talk about the dataset, I have downloaded [Used Car Price Dataset](https://www.kaggle.com/datasets/rishabhkarn/used-car-dataset) from Kaggle website. The dataset contains `13` feature/independent variables($x_i$) and a target/dependant variable($y$), from those 13 feature/independant variables, I will be using 5 feature/independent variables they are `kms_driven` ,`mileage(kmpl)`, `engine(cc)`, `max_power(bhp)` and `torque(Nm). ` and `price(in lakhs)` as the target variable.<br><br>\n",
        "\n",
        "\n",
        "kms_driven   | mileage(kmpl)| engine(cc) | max_power(bhp) | torque(Nm) | price(in lakhs)\n",
        "-------------|-------------|------------|----------------|------------|-------------\n",
        " 56000       |   7.81      |2996        |\t   2996        |     333    |       63.75\n",
        " 30615       |  17.4      |    999     |     999        |     9863   |       8.99\n",
        "24000        |  20.68     |    1995    |     1995       |      188   |       23.75\n",
        "18378        |  16.5      |    1353    |     1353       |     13808  |       13.56\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "Here $x$'s are four-dimensional vector in $\\mathbb{R}^5$. For instance, $x_1^{(i)}$ is the `kms_driven`, $x_2^{(i)}$ is the `mileage(kmpl)`, $x_3^{(i)}$ is the `engine(cc)`, $x_4^{(i)}$ is the `max_power(bhp)`, and $x_4^{(i)}$ is the `torque(Nm)` of the $i$-th house in the training set\n"
      ],
      "metadata": {
        "id": "tKxdr-fuFbrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYzc0PHhdiVS",
        "outputId": "79708052-51ed-4849-8408-b8b25c9b275f"
      },
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AdPuJbxdmQt",
        "outputId": "622153ba-39a5-42a6-db42-6ce1f67b8016"
      },
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGSGdkCWdpYI",
        "outputId": "9ae5648d-beb8-4b1b-c21d-b843ee2ae736"
      },
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdb_data.csv  Used_Car_Dataset.csv  yelp.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (12.0, 9.0)"
      ],
      "metadata": {
        "id": "oVlIHFhKfn2K"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Used_Car_Dataset.csv\")"
      ],
      "metadata": {
        "id": "7uSh7Ekpfq_-"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop all other columsn except mileage(kmpl), engine(cc), max_power(bhp), torque(Nm), price(in lakhs)\n",
        "# sort the dataframe based on the price(in lakhs) column and only 100 samples are choosen\n",
        "df = df[:10]\n",
        "df = df[['kms_driven','mileage(kmpl)', 'engine(cc)', 'max_power(bhp)', 'torque(Nm)', 'price(in lakhs)']]\n",
        "df['kms_driven'] = df['kms_driven']/1000\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0BgtVUDfv87",
        "outputId": "6a4ff34e-9534-447c-b4bc-dec5847d99ef"
      },
      "execution_count": 370,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10 entries, 0 to 9\n",
            "Data columns (total 6 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   kms_driven       10 non-null     float64\n",
            " 1   mileage(kmpl)    10 non-null     float64\n",
            " 2   engine(cc)       10 non-null     float64\n",
            " 3   max_power(bhp)   10 non-null     float64\n",
            " 4   torque(Nm)       10 non-null     float64\n",
            " 5   price(in lakhs)  10 non-null     float64\n",
            "dtypes: float64(6)\n",
            "memory usage: 608.0 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to replicate the values across, lets set the random seed\n",
        "np.random.seed(seed=123456)\n",
        "X = df[['kms_driven','mileage(kmpl)', 'engine(cc)', 'max_power(bhp)', 'torque(Nm)']].to_numpy()/1000\n",
        "y = df[['price(in lakhs)']].to_numpy()\n",
        "#We then need to add a feature of “1” concatenating it with the dataset we already have and also add q to the vector m.\n",
        "X = np.concatenate([X , np.ones((X.shape[0],1))], axis = 1)"
      ],
      "metadata": {
        "id": "iKAZ7ICOf1Oj"
      },
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform supervised learning, we will represent $y$ as  the functions/hypothese $h$ or as a linear function of $x$.\n",
        "\n",
        "$$h_{\\theta}(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 + \\theta_4x_4 + \\theta_5x_5$$\n",
        "\n",
        "Here, the $\\theta_i$'s are the parameters (also called wights) parameterizing the space of linear functions mapping $\\mathcal{X}$ $\\to$ $\\mathcal{Y}$. For simplifying the notation, we drop $\\theta$ and use $x_0=1$(intercept term).\n",
        "$$h(x) = \\sum_{i=0}^d\\theta_ix_i = \\theta^Tx$$\n",
        "\n",
        "on the right-hand side both $\\theta$ and $x$ are both vectors and d is the no. of feature/input/independent variables(not counting $x_0$).\n",
        "\n",
        "\n",
        "\n",
        "Now, given a training set, how do we pick, or learn, the parameters $\\theta$. One reasonable method is to make $h(x)$ close to $y$, atleast for the training examples we have. To formalize this, we will define a function that measures, for each value of the $\\theta$'s, how close the $h(x^{(i)})$'s are to the correspoding $y^{(i)}$'s. We define the **cost function**:\n",
        "$$J(\\theta) = \\frac{1}{2}\\sum_{i=1}^n(h_{\\theta}(x^{(i)})-y^{(i)})^2.$$\n",
        "\n"
      ],
      "metadata": {
        "id": "bJ2UIvD9PPqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# contains the theta random theta values from θ1, θ2, θ3, θ4, θ5\n",
        "θ_1_5 = np.random.rand(5,1)\n",
        "\n",
        "# creating the θ0(theta zero or intecept)\n",
        "θ0 = np.random.rand(1)\n",
        "\n",
        "# complete hθ\n",
        "hθ = np.concatenate([θ_1_5,q.reshape(1,-1)],axis = 0)"
      ],
      "metadata": {
        "id": "dWakP7Uef_ZO"
      },
      "execution_count": 391,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(hθ.shape)\n",
        "print(hθ)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-EXZbZagUjd",
        "outputId": "90a145c0-edfb-4227-f011-c6105b7d8772"
      },
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 1)\n",
            "[[0.12696983]\n",
            " [0.96671784]\n",
            " [0.26047601]\n",
            " [0.89723652]\n",
            " [0.37674972]\n",
            " [0.19452593]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LMS algorithm\n"
      ],
      "metadata": {
        "id": "Ficrjfy0N_SN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to choose $\\theta$ so as to minimize $J(\\theta)$. To do so, let's use a search algorithm that starts with some \"initial guess\" for $\\theta$, and that repeatedly changes $\\theta$ to make $J(\\theta)$ smaller, until hopefully we converge to a value of $\\theta$ that minimizes $J(\\theta)$. Specifically, lets consider the **gradient descent** algorithm, which starts with some initial $\\theta$, and repeatedly performs the update:\n",
        "$$\\theta_j:=\\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta). $$\n",
        "(This update is simultaneously performed for all values of $j$= 0,...,d.)\n",
        "\n",
        "\n",
        "Here, $\\alpha$ is called the **learning rate**. This is a very natural algorithm that repeatedly takes a step in the direction of steepest descrease of $J$.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fagHUGT-lSm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we have defined the cost fuction jθ, which is modelled after the mean square error\n",
        "def mean_squared_error(X, y, hθ):\n",
        "    hx = X @ hθ\n",
        "    jθ = (1/2) * np.sum(((hx - y)**2), axis = 0)/len(X)\n",
        "    print(jθ, \"mse\")\n",
        "    return jθ"
      ],
      "metadata": {
        "id": "Gw66Z5UGj5cJ"
      },
      "execution_count": 374,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to implement this algorithm, we have to work out what is the partial derivative term on the right hand side. Lets first work it out for the case of it we have only one training example $(x, y)$, so that we can neglect the sum in the definition of $\\textit{J}$. We have:\n",
        "\n",
        "\n",
        "$$ \\frac{\\partial}{\\partial\\theta_j}J(\\theta) = \\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2}(h_\\theta(x) - y)^2$$\n",
        "$$ = 2.\\frac{1}{2}(h_\\theta(x)-y).\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y))$$\n",
        "$$=(h_\\theta(x)-y).\\frac{\\partial}{\\partial\\theta_j}(\\sum_{i=0}^d\\theta_ix_i-y)$$\n",
        "$$=(h_\\theta(x)-y)x_j$$\n",
        "for a single training example, this gives the update rule:\n",
        "$$\\theta_j:= \\theta_j +\\alpha(y^{(i)}-h_\\theta(x^{(i)}))x_j^{(i)}. $$\n",
        "\n",
        "This rule is called the **LMS** update rule (LMS stands for \"least mean squares\"), and is also known as the **Widrow-Hoff** learning rule. This rule has several properties that seem natural and intuitive. For instance, the magnitude of the update is proportial to the error term $(y^{(i)}-h_\\theta(x^{(i)}))$; thus for instance, if we are encountering a training example on which our prediction nearly matches the actual value of $y^{(i)}$, then we find that there is little need to change the parameters; in contrast, a larger change to the parameters will be made if our prediction $h_\\theta(x^(i)$ has a large error (i.e, if it is very far from $y^{(i)}$).\n",
        "\n",
        "We'd derived the LMS rule for when there was only a single training example. There are two ways to modify this method for a training set of more than one example. The first is replace it with the following algorithm:\n",
        "\n",
        "repeat until convergence $ \\{ $\n",
        "$$\\theta_j := \\theta_j + \\alpha\\sum_{i=i}^n(y^{(i)} - h_\\theta(x^{(i)}))x_j^{(i)}, \\text{(for every $j$)  (1.1)} $$\n",
        "$ \\}$"
      ],
      "metadata": {
        "id": "6gDKydpLlOG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partial_derivative(X_batch, y_batch, hθ):\n",
        "    y_pred = X_batch @ hθ\n",
        "    n = len(X_batch)\n",
        "\n",
        "    df_dhθ = -(2/n) * (X_batch.T @ (y_batch - y_pred))\n",
        "    df_dhθ = df_dhθ.reshape(len(df_dhθ), -1)\n",
        "    return df_dhθ"
      ],
      "metadata": {
        "id": "kBJ1EFaJI7c9"
      },
      "execution_count": 375,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_batch(X, y, batch_size, lr, epochs, hθ):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # random initial statistics\n",
        "        # if epoch == 0:\n",
        "        #     hθ = np.random.randint(low = 1, high = 20,size = (X.shape[1], 1))\n",
        "\n",
        "        # shuffle X and y using same permutation\n",
        "        indices = np.arange(X.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        X = X[indices]\n",
        "        y = y[indices]\n",
        "\n",
        "        # store cumulative derivative\n",
        "        cumulative_derivative = np.zeros((X.shape[1], 1))\n",
        "\n",
        "        for batch in range(len(X)//batch_size):\n",
        "            start = batch*batch_size\n",
        "            stop = (batch*batch_size) + batch_size\n",
        "\n",
        "            X_batch = X[start:stop]\n",
        "            y_batch = y[start:stop]\n",
        "            parti = partial_derivative(X_batch, y_batch, hθ)\n",
        "            cumulative_derivative = cumulative_derivative + parti\n",
        "\n",
        "            #updating rule\n",
        "            hθ = hθ - (lr * cumulative_derivative)\n",
        "        #print(f\"m_stat: {m_stat}\")\n",
        "        print(f\"epoch: {epoch} ----> MSE: {mean_squared_error(X, y, hθ)}\")\n",
        "    return hθ\n"
      ],
      "metadata": {
        "id": "kUcKLgWKGGoX"
      },
      "execution_count": 376,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "lr = 0.001\n",
        "epochs = 100\n",
        "\n",
        "hθ = training_batch(X,y, batch_size,lr,epochs, hθ)\n",
        "hθ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KQd59-IJhzP",
        "outputId": "4c5ef19d-8f5d-45e7-a19d-499c2ad50f66"
      },
      "execution_count": 389,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[202.65497215] mse\n",
            "epoch: 0 ----> MSE: [202.65497215]\n",
            "[189.21275634] mse\n",
            "epoch: 1 ----> MSE: [189.21275634]\n",
            "[184.58716944] mse\n",
            "epoch: 2 ----> MSE: [184.58716944]\n",
            "[180.11673439] mse\n",
            "epoch: 3 ----> MSE: [180.11673439]\n",
            "[175.88641771] mse\n",
            "epoch: 4 ----> MSE: [175.88641771]\n",
            "[169.56503536] mse\n",
            "epoch: 5 ----> MSE: [169.56503536]\n",
            "[163.03116902] mse\n",
            "epoch: 6 ----> MSE: [163.03116902]\n",
            "[159.94591485] mse\n",
            "epoch: 7 ----> MSE: [159.94591485]\n",
            "[156.35049897] mse\n",
            "epoch: 8 ----> MSE: [156.35049897]\n",
            "[153.28434207] mse\n",
            "epoch: 9 ----> MSE: [153.28434207]\n",
            "[149.94844463] mse\n",
            "epoch: 10 ----> MSE: [149.94844463]\n",
            "[144.53568449] mse\n",
            "epoch: 11 ----> MSE: [144.53568449]\n",
            "[139.85232592] mse\n",
            "epoch: 12 ----> MSE: [139.85232592]\n",
            "[135.41322441] mse\n",
            "epoch: 13 ----> MSE: [135.41322441]\n",
            "[133.12959572] mse\n",
            "epoch: 14 ----> MSE: [133.12959572]\n",
            "[128.86541199] mse\n",
            "epoch: 15 ----> MSE: [128.86541199]\n",
            "[126.44854131] mse\n",
            "epoch: 16 ----> MSE: [126.44854131]\n",
            "[124.04623995] mse\n",
            "epoch: 17 ----> MSE: [124.04623995]\n",
            "[120.08567146] mse\n",
            "epoch: 18 ----> MSE: [120.08567146]\n",
            "[117.96051215] mse\n",
            "epoch: 19 ----> MSE: [117.96051215]\n",
            "[114.59687472] mse\n",
            "epoch: 20 ----> MSE: [114.59687472]\n",
            "[112.71769331] mse\n",
            "epoch: 21 ----> MSE: [112.71769331]\n",
            "[111.24932515] mse\n",
            "epoch: 22 ----> MSE: [111.24932515]\n",
            "[108.24236042] mse\n",
            "epoch: 23 ----> MSE: [108.24236042]\n",
            "[106.69695147] mse\n",
            "epoch: 24 ----> MSE: [106.69695147]\n",
            "[105.11472098] mse\n",
            "epoch: 25 ----> MSE: [105.11472098]\n",
            "[103.38223269] mse\n",
            "epoch: 26 ----> MSE: [103.38223269]\n",
            "[100.63913844] mse\n",
            "epoch: 27 ----> MSE: [100.63913844]\n",
            "[98.00331989] mse\n",
            "epoch: 28 ----> MSE: [98.00331989]\n",
            "[95.40613968] mse\n",
            "epoch: 29 ----> MSE: [95.40613968]\n",
            "[94.24541493] mse\n",
            "epoch: 30 ----> MSE: [94.24541493]\n",
            "[93.05909381] mse\n",
            "epoch: 31 ----> MSE: [93.05909381]\n",
            "[91.9779195] mse\n",
            "epoch: 32 ----> MSE: [91.9779195]\n",
            "[89.74742913] mse\n",
            "epoch: 33 ----> MSE: [89.74742913]\n",
            "[88.62748045] mse\n",
            "epoch: 34 ----> MSE: [88.62748045]\n",
            "[87.6111604] mse\n",
            "epoch: 35 ----> MSE: [87.6111604]\n",
            "[86.8411523] mse\n",
            "epoch: 36 ----> MSE: [86.8411523]\n",
            "[84.81248279] mse\n",
            "epoch: 37 ----> MSE: [84.81248279]\n",
            "[83.25185421] mse\n",
            "epoch: 38 ----> MSE: [83.25185421]\n",
            "[82.35222371] mse\n",
            "epoch: 39 ----> MSE: [82.35222371]\n",
            "[81.59063038] mse\n",
            "epoch: 40 ----> MSE: [81.59063038]\n",
            "[80.90712124] mse\n",
            "epoch: 41 ----> MSE: [80.90712124]\n",
            "[79.31068108] mse\n",
            "epoch: 42 ----> MSE: [79.31068108]\n",
            "[78.01817776] mse\n",
            "epoch: 43 ----> MSE: [78.01817776]\n",
            "[76.42565526] mse\n",
            "epoch: 44 ----> MSE: [76.42565526]\n",
            "[75.08544197] mse\n",
            "epoch: 45 ----> MSE: [75.08544197]\n",
            "[74.63734031] mse\n",
            "epoch: 46 ----> MSE: [74.63734031]\n",
            "[73.3438174] mse\n",
            "epoch: 47 ----> MSE: [73.3438174]\n",
            "[73.0022098] mse\n",
            "epoch: 48 ----> MSE: [73.0022098]\n",
            "[72.56740591] mse\n",
            "epoch: 49 ----> MSE: [72.56740591]\n",
            "[71.38135345] mse\n",
            "epoch: 50 ----> MSE: [71.38135345]\n",
            "[71.02120093] mse\n",
            "epoch: 51 ----> MSE: [71.02120093]\n",
            "[70.48073061] mse\n",
            "epoch: 52 ----> MSE: [70.48073061]\n",
            "[70.2111235] mse\n",
            "epoch: 53 ----> MSE: [70.2111235]\n",
            "[69.28695399] mse\n",
            "epoch: 54 ----> MSE: [69.28695399]\n",
            "[68.74965599] mse\n",
            "epoch: 55 ----> MSE: [68.74965599]\n",
            "[68.55738707] mse\n",
            "epoch: 56 ----> MSE: [68.55738707]\n",
            "[67.98156079] mse\n",
            "epoch: 57 ----> MSE: [67.98156079]\n",
            "[67.75034309] mse\n",
            "epoch: 58 ----> MSE: [67.75034309]\n",
            "[66.94718668] mse\n",
            "epoch: 59 ----> MSE: [66.94718668]\n",
            "[66.17996703] mse\n",
            "epoch: 60 ----> MSE: [66.17996703]\n",
            "[65.9363166] mse\n",
            "epoch: 61 ----> MSE: [65.9363166]\n",
            "[65.15466067] mse\n",
            "epoch: 62 ----> MSE: [65.15466067]\n",
            "[64.54348504] mse\n",
            "epoch: 63 ----> MSE: [64.54348504]\n",
            "[64.3311657] mse\n",
            "epoch: 64 ----> MSE: [64.3311657]\n",
            "[63.65771018] mse\n",
            "epoch: 65 ----> MSE: [63.65771018]\n",
            "[63.00801017] mse\n",
            "epoch: 66 ----> MSE: [63.00801017]\n",
            "[62.8585406] mse\n",
            "epoch: 67 ----> MSE: [62.8585406]\n",
            "[62.69163766] mse\n",
            "epoch: 68 ----> MSE: [62.69163766]\n",
            "[62.62340409] mse\n",
            "epoch: 69 ----> MSE: [62.62340409]\n",
            "[62.06594866] mse\n",
            "epoch: 70 ----> MSE: [62.06594866]\n",
            "[61.91467532] mse\n",
            "epoch: 71 ----> MSE: [61.91467532]\n",
            "[61.69216998] mse\n",
            "epoch: 72 ----> MSE: [61.69216998]\n",
            "[61.48427503] mse\n",
            "epoch: 73 ----> MSE: [61.48427503]\n",
            "[61.32692659] mse\n",
            "epoch: 74 ----> MSE: [61.32692659]\n",
            "[61.3641641] mse\n",
            "epoch: 75 ----> MSE: [61.3641641]\n",
            "[60.718725] mse\n",
            "epoch: 76 ----> MSE: [60.718725]\n",
            "[60.23310932] mse\n",
            "epoch: 77 ----> MSE: [60.23310932]\n",
            "[60.12082824] mse\n",
            "epoch: 78 ----> MSE: [60.12082824]\n",
            "[60.04589542] mse\n",
            "epoch: 79 ----> MSE: [60.04589542]\n",
            "[59.61125362] mse\n",
            "epoch: 80 ----> MSE: [59.61125362]\n",
            "[59.40582263] mse\n",
            "epoch: 81 ----> MSE: [59.40582263]\n",
            "[59.06515252] mse\n",
            "epoch: 82 ----> MSE: [59.06515252]\n",
            "[58.76123584] mse\n",
            "epoch: 83 ----> MSE: [58.76123584]\n",
            "[58.63402458] mse\n",
            "epoch: 84 ----> MSE: [58.63402458]\n",
            "[58.41884459] mse\n",
            "epoch: 85 ----> MSE: [58.41884459]\n",
            "[57.88337235] mse\n",
            "epoch: 86 ----> MSE: [57.88337235]\n",
            "[57.74025613] mse\n",
            "epoch: 87 ----> MSE: [57.74025613]\n",
            "[57.39045707] mse\n",
            "epoch: 88 ----> MSE: [57.39045707]\n",
            "[57.13608809] mse\n",
            "epoch: 89 ----> MSE: [57.13608809]\n",
            "[57.11569231] mse\n",
            "epoch: 90 ----> MSE: [57.11569231]\n",
            "[56.86827663] mse\n",
            "epoch: 91 ----> MSE: [56.86827663]\n",
            "[56.73050786] mse\n",
            "epoch: 92 ----> MSE: [56.73050786]\n",
            "[56.67937041] mse\n",
            "epoch: 93 ----> MSE: [56.67937041]\n",
            "[56.41941197] mse\n",
            "epoch: 94 ----> MSE: [56.41941197]\n",
            "[56.19792199] mse\n",
            "epoch: 95 ----> MSE: [56.19792199]\n",
            "[56.1476903] mse\n",
            "epoch: 96 ----> MSE: [56.1476903]\n",
            "[56.16220992] mse\n",
            "epoch: 97 ----> MSE: [56.16220992]\n",
            "[56.12486224] mse\n",
            "epoch: 98 ----> MSE: [56.12486224]\n",
            "[55.85557477] mse\n",
            "epoch: 99 ----> MSE: [55.85557477]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.19711947],\n",
              "       [ 0.71957322],\n",
              "       [ 6.07595809],\n",
              "       [ 6.40026392],\n",
              "       [-0.06673339],\n",
              "       [ 1.00791242]])"
            ]
          },
          "metadata": {},
          "execution_count": 389
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A-mD_LgVnFOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By grouping the updates of the coordinates into an update of the vector $\\theta$, we can rewrite update (1.1) in a slightly more succing way:\n",
        "$$ \\theta := \\theta + \\alpha\\sum_{i=1}^n(y^{(i)} - h_\\theta(x^{(i)}))x^{(i)}$$\n",
        "This is simply gradient descent on the original cost funtion $\\textit{J}$. This method looks ar every example in the entire training set on every step, and is called **batch gradient descent**. Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assumming the learning rate $\\alpha$ is not too large) to the global minimum. Indeed, $\\textit{J}$ is a convex quadratic funtion. Here is an examlpe of gradient descent as it run to minimize a quadratic function.\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://github.com/0shankart/MachineLearningNotes/blob/main/TraditionalML/images/batch_gradient_descent_global_minimum.png?raw=true\" width=\"40%\"/>\n",
        "  <figcaption>Ref: https://cs229.stanford.edu/notes2021fall/cs229-notes1.pdf</figcaption>\n",
        "</figure>\n",
        " <br>\n",
        " <br>The ellipses shown above are the countours of a quadratiic function. Also shown is the trajectory inspired by gradient descent, which was initialized for $θ$ at (0.12696983, 0.96671784, 0.26047601, 0.89723652, 0.37674972, 0.19452593). The $x$'s in the figure (joined by straight lines) mark the successive values of $\\theta$ that gradient descent might went through and found  (0.19711947, 0.71957322, 6.07595809, 6.40026392, -0.06673339, 1.00791242)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nGsaG2n1n-RK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s-EyPn2Nn-Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is an alternative to batch gradient descent that also works very well. Consider the folling algorithm:<br><br>$\\text{Loop }  \\{ $\n",
        "\n",
        " $ \\text{ for $i$ = 1 to n, } \\{ $\n",
        " $$\\theta^j := \\theta_j + \\alpha(y^{(i)} - h_\\theta(x^{(i)}))x_j^{(i)}, \\text{ (for every $j$)  (1.2) }$$\n",
        " $\\}$\n",
        "\n",
        " $\\}$\n",
        "<br>\n",
        "By grouping the updates of the coordinates into an update of the vector $\\theta$, we can rewrite update (1.2) in a slightly more succing way:\n",
        "$$\\theta := \\theta + \\alpha(y^{(i)} - h_\\theta(x^{(i)}))x^{(i)}$$\n",
        "<br>\n",
        "In this algorithm, we repeatedly run through the training set, and each time we encounter a training example, we update the parameters according to the gradient of the error with respect to that single training examlpe only. This algorithm is called **stochastic gradient descent** (also **incremental gradient descent**). Whereas batch gradient descent has to scan through the entire training set before taking a single step$-$a cpst;u p[eratopm of n is large$-$stochastic gradient descent can start making progress right away and continues to ,make progress with each example it looks at. Often, stochastic gradient descent gets $\\theta$ \"close to the minimum much faster than batch gradient descent. (Note however that it may never \"converge\" to the minimum, and the parameters $\\theta$ will keep oscillating around the minimum of the $J(\\theta)$; but in practise most of the values near the minimum will be reasonably good approximations to the true minimum. By slowly letting the learning rate $\\alpha$ decrease to zero as the algorithm runs, it is also possible to ensure that the parameters will converge to the global minimum rather than merely oscillate around the minimum.) For these reasons, particularly when the training set is large, stochastic gradient descent is often preferred voer batch gradient descent.\n",
        "\n"
      ],
      "metadata": {
        "id": "fyOPihg2hQdW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_fdioDNcmOlC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
