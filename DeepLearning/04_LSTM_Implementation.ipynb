{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTO7r8oy358v"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/files"
      ],
      "metadata": {
        "id": "aEclUmcXdyD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam Optimization<br>\n",
        "It is short for \"Adaptive Moment Estimation\", is an interative optimizatoin algorithm used to minimize the loss function during the training of neural networks. It is a mix of RMSprop and Stochastic Gradient Descent with momentum.\n",
        "$$\\text{SGD with momentum } + \\text{RMSprop}[\\text{}]$$\n",
        "\n",
        "<br>\n",
        "**References**:\n",
        "1.   https://www.analyticsvidhya.com/blog/2023/09/what-is-adam-optimizer/\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SAlge_BmxFAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in this example we will use the input.txt downloaded from the\n",
        "#https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFUv5UeEwFCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM"
      ],
      "metadata": {
        "id": "Xs1n0s1C7WPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM consists of two paths, its long-term path(memory state or context state) and other is short-term path(hidden state). We remove the unimportant information from the context withwith **Forget Gate** in the long-term path. We update the long-term path based on the input and short-term path using the **Input Gate** & **Candidate Gate**. When the important information is preserved in the long-short path, that information can be removed from the short-term path using **Output Gate** & **Final Gate**."
      ],
      "metadata": {
        "id": "truGuYuZ79TI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Forget Gate"
      ],
      "metadata": {
        "id": "dt2m_Okt7WMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is used to forget unimportant information from the Context state. LSTM can remember limited information. It can be achieved by below forget gate definition <br>\n",
        "$$F_t = \\sigma(W_fZ_t + b_f)$$\n",
        "Forget gate is defined as sigmoid of the weight matrix ($W_f$) times the concatenation of the previous hidden state and the current input ($Z_t$), plus the bias ($b_f$)."
      ],
      "metadata": {
        "id": "SqfBih9B987i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Candidate Gate"
      ],
      "metadata": {
        "id": "kWAMXrqY7WIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is used to apply the new information to the Context state and its is defined as<br>\n",
        "$$C_t = \\tanh(W_cZ_t + b_c)$$\n",
        "Candidate gate is defined as tanh of weight matrix ($W_c$) times the concatenation of the previous hidden state and the current input ($Z_f$), plus the bias ($b_c$)."
      ],
      "metadata": {
        "id": "ZDFPYOkS_atA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Input Gate"
      ],
      "metadata": {
        "id": "wJ4jsAMj7WFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It determines how much of the information should be added to the context state. It is defined using the sigmoid function,<br>\n",
        "$$I_t = \\sigma (W_iZ_t + b_i)$$\n",
        "Input gate is dedined as sigmoid of weight matrix $W_i$ times the concatenation of the previous hidden state and current input($Z_t$) and the bias($b_c$)."
      ],
      "metadata": {
        "id": "Ff6PH3gWAoqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ouput Gate"
      ],
      "metadata": {
        "id": "rM4sGZE97WB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like the input gate, this is used to determine the amount of information that should be remembered across the hidden state.<br>\n",
        "$$O_t = \\sigma(W_oZ_t + b_o)$$\n",
        "Output gate is defined as sigmoid of the weights matrix ($W_o$) and  the concatenated input and hidden state ($Z_t$) and bias ($b_o$)."
      ],
      "metadata": {
        "id": "ZtjUTrLVDOZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Final Gate"
      ],
      "metadata": {
        "id": "fMhHCnp77V-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the output of the lstm network, it is defined as <br>\n",
        "$$Y_t = W_yHS_t + b_y$$\n",
        "It is the simple wight matrix($W_y$) multiplied with hidden state($HS_t$) and bias ($b_y$)"
      ],
      "metadata": {
        "id": "7WKSX1SH7V6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Complete Mathematical Equations"
      ],
      "metadata": {
        "id": "bUvXPe8l7V13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\begin{align*}\n",
        "F_t &= \\sigma(W_fZ_t + b_f) \\tag{Forget Gate Definition}\\\\\n",
        "I_t &= \\sigma(W_iZ_t + b_i) \\tag{Input Gate Definition}\\\\\n",
        "C_t &= \\tanh(W_cZ_t + b_c) \\tag{Candidate Gate Definition}\\\\\n",
        "O_t &= \\sigma(W_oZ_t + b_o) \\tag{ouput Gate Definition}\\\\\n",
        "CS_t &= F_t \\otimes CS_{t-1} + I_t \\otimes C_t \\tag{Cell State Definition}\\\\\n",
        "HS_t &= O_t \\otimes \\tanh(CS_t) \\tag{Hidden State Definition}\\\\\n",
        "Y_t &= W_y \\otimes HS_t + b_y \\tag{Final Gate Definition}\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "ETUy4SukLDtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\begin{align*}\n",
        "\\dfrac{d}{dx}\\tanh(x) &= 1 - \\tanh^2 (x) \\tag{Tanh derivative definition}\\\\\n",
        "\\dfrac{d}{dx} \\sigma(x) &= \\sigma(x)(1 - \\sigma(x)) \\tag{Sigmoid derivative definition}\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "88mHAOGQM-OW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AfcDfWv6M-GC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OhR-AFZWM-B8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "KjZ0SJDdz0zp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Imports #####\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "##### Data #####\n",
        "data = \"\"\"To be, or not to be, that is the question: Whether \\\n",
        "'tis nobler in the mind to suffer The slings and arrows of ou\\\n",
        "trageous fortune, Or to take arms against a sea of troubles A\\\n",
        "nd by opposing end them. To die—to sleep, No more; and by a s\\\n",
        "leep to say we end The heart-ache and the thousand natural sh\\\n",
        "ocks That flesh is heir to: 'tis a consummation Devoutly to b\\\n",
        "e wish'd. To die, to sleep; To sleep, perchance to dream—ay, \\\n",
        "there's the rub: For in that sleep of death what dreams may c\\\n",
        "ome, When we have shuffled off this mortal coil, Must give us\\\n",
        " pause—there's the respect That makes calamity of so long lif\\\n",
        "e. For who would bear the whips and scorns of time, Th'oppres\\\n",
        "sor's wrong, the proud man's contumely, The pangs of dispriz'\\\n",
        "d love, the law's delay, The insolence of office, and the spu\\\n",
        "rns That patient merit of th'unworthy takes, When he himself \\\n",
        "might his quietus make\"\"\".lower()"
      ],
      "metadata": {
        "id": "GZG0-rNz0A4d"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = set(data)\n",
        "\n",
        "data_size, char_size = len(data), len(chars)\n",
        "\n",
        "print(f'Data Size: {data_size}, Char Size: {len(chars)}')\n",
        "\n",
        "char_to_idx = {c:i for i, c in enumerate(chars)}\n",
        "idx_to_char = {i:c for i, c in enumerate(chars)}\n",
        "\n",
        "train_X, train_y = data[:-1], data[1:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjnwy7dhQLzM",
        "outputId": "decb88e5-f2a2-4dce-b384-595d102d11a7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Size: 866, Char Size: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Helper Functions #####\n",
        "def oneHotEncode(text):\n",
        "    output =np.zeros((char_size, 1))\n",
        "    output[char_to_idx[text]] = 1\n",
        "    return output\n",
        "\n",
        "# Xavier Normalized Initialization\n",
        "def initWeights(input_size, output_size):\n",
        "    return np.random.uniform(-1, 1, (output_size, input_size)) * np.sqrt(6/(input_size + output_size))"
      ],
      "metadata": {
        "id": "t8ZZTD6kQ4jx"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Activation Functions ####\n",
        "def sigmoid(input, derivative = False):\n",
        "    if derivative:\n",
        "        return input * (1 - input)\n",
        "\n",
        "    return 1 /(1 + np.exp(-input))\n",
        "\n",
        "\n",
        "def tanh(input, derivative = False):\n",
        "    if derivative:\n",
        "        return 1 - input ** 2\n",
        "    return np.tanh(input)\n",
        "\n",
        "\n",
        "def softmax(input):\n",
        "    return np.exp(input)/np.sum(np.exp(input))\n",
        "\n"
      ],
      "metadata": {
        "id": "HSjC91ZHQ4gn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Long Short-Term Memory Network Class ####\n",
        "class LSTM:\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_epochs, learning_rate):\n",
        "        # Hyperparameters\n",
        "        self.learning_rate = learning_rate\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "        # Forget Gate\n",
        "        self.wf = initWeights(input_size, hidden_size)\n",
        "        self.bf = np.zeros((hidden_size, 1))\n",
        "\n",
        "        # Input Gate\n",
        "        self.wi = initWeights(input_size, hidden_size)\n",
        "        self.bi = np.zeros((hidden_size, 1))\n",
        "\n",
        "        # Candidate Gate\n",
        "        self.wc = initWeights(input_size, hidden_size)\n",
        "        self.bc = np.zeros((hidden_size, 1))\n",
        "\n",
        "        # Output Gate\n",
        "        self.wo = initWeights(input_size, hidden_size)\n",
        "        self.bo = np.zeros((hidden_size, 1))\n",
        "\n",
        "        # Final Gate\n",
        "        self.wy = initWeights(hidden_size, output_size)\n",
        "        self.by = np.zeros((output_size, 1))\n",
        "\n",
        "    # Reset Network Memory\n",
        "    def reset(self):\n",
        "        self.concat_inputs ={}\n",
        "\n",
        "        self.hidden_states = {-1 : np.zeros((self.hidden_size, 1))}\n",
        "        self.cell_states = {-1 : np.zeros((self.hidden_size, 1))}\n",
        "\n",
        "        self.activation_outputs = {}\n",
        "        self.candidate_gates = {}\n",
        "        self.output_gates = {}\n",
        "        self.forget_gates = {}\n",
        "        self.input_gates = {}\n",
        "        self.outputs = {}\n",
        "\n",
        "    # Forward Propagation\n",
        "    def forward(self, inputs):\n",
        "        self.reset()\n",
        "\n",
        "        outputs = []\n",
        "        for q in range(len(inputs)):\n",
        "            self.concat_inputs[q] = np.concatenate((self.hidden_states[q - 1], inputs[q]))\n",
        "\n",
        "            self.forget_gates[q] = sigmoid(np.dot(self.wf, self.concat_inputs[q]) + self.bf)\n",
        "            self.input_gates[q] = sigmoid(np.dot(self.wi, self.concat_inputs[q]) + self.bi)\n",
        "            self.candidate_gates[q] = tanh(np.dot(self.wc, self.concat_inputs[q]) + self.bc)\n",
        "            self.output_gates[q] = sigmoid(np.dot(self.wo, self.concat_inputs[q]) + self.bo)\n",
        "\n",
        "            self.cell_states[q] = self.forget_gates[q] * self.cell_states[q - 1] + self.input_gates[q] * self.candidate_gates[q]\n",
        "            self.hidden_states[q] = self.output_gates[q] * tanh(self.cell_states[q])\n",
        "\n",
        "            outputs += [np.dot(self.wy, self.hidden_states[q]) + self.by]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    # Backward Propagation\n",
        "    def backward(self, errors, inputs):\n",
        "        d_wf, d_bf = 0, 0\n",
        "        d_wi, d_bi = 0, 0\n",
        "        d_wc, d_bc = 0, 0\n",
        "        d_wo, d_bo = 0, 0\n",
        "        d_wy, d_by = 0, 0\n",
        "\n",
        "        dh_next, dc_next = np.zeros_like(self.hidden_states[0]), np.zeros_like(self.cell_states[0])\n",
        "        for q in reversed(range(len(inputs))):\n",
        "            error = errors[q]\n",
        "\n",
        "            # Final Gate Weights and Biases Errors\n",
        "            d_wy += np.dot(error, self.hidden_states[q].T)\n",
        "            d_by += error\n",
        "\n",
        "            # Hidden State Error\n",
        "            d_hs = np.dot(self.wy.T, error) + dh_next\n",
        "\n",
        "            # Output Gate Weights and Biases Errors\n",
        "            d_o = tanh(self.cell_states[q]) * d_hs * sigmoid(self.output_gates[q], derivative = True)\n",
        "            d_wo += np.dot(d_o, inputs[q].T)\n",
        "            d_bo += d_o\n",
        "\n",
        "            # Cell State Error\n",
        "            d_cs = tanh(tanh(self.cell_states[q]), derivative = True) * self.output_gates[q] * d_hs + dc_next\n",
        "\n",
        "            # Forget Gate Weights and Biases Errors\n",
        "            d_f = d_cs * self.cell_states[q - 1] * sigmoid(self.forget_gates[q], derivative = True)\n",
        "            d_wf += np.dot(d_f, inputs[q].T)\n",
        "            d_bf += d_f\n",
        "\n",
        "            # Input Gate Weights and Biases Errors\n",
        "            d_i = d_cs * self.candidate_gates[q] * sigmoid(self.input_gates[q], derivative = True)\n",
        "            d_wi += np.dot(d_i, inputs[q].T)\n",
        "            d_bi += d_i\n",
        "\n",
        "            # Candidate Gate Weights and Biases Errors\n",
        "            d_c = d_cs * self.input_gates[q] * tanh(self.candidate_gates[q], derivative = True)\n",
        "            d_wc += np.dot(d_c, inputs[q].T)\n",
        "            d_bc += d_c\n",
        "\n",
        "            # Concatenated Input Error (Sum of Error at Each Gate!)\n",
        "            d_z = np.dot(self.wf.T, d_f) + np.dot(self.wi.T, d_i) + np.dot(self.wc.T, d_c) + np.dot(self.wo.T, d_o)\n",
        "\n",
        "            # Error of Hidden State and Cell State at Next Time Step\n",
        "            dh_next = d_z[:self.hidden_size, :]\n",
        "            dc_next = self.forget_gates[q] * d_cs\n",
        "\n",
        "        for d_ in (d_wf, d_bf, d_wi, d_bi, d_wc, d_bc, d_wo, d_bo, d_wy, d_by):\n",
        "            np.clip(d_, -1, 1, out = d_)\n",
        "\n",
        "        self.wf += d_wf * self.learning_rate\n",
        "        self.bf += d_bf * self.learning_rate\n",
        "\n",
        "        self.wi += d_wi * self.learning_rate\n",
        "        self.bi += d_bi * self.learning_rate\n",
        "\n",
        "        self.wc += d_wc * self.learning_rate\n",
        "        self.bc += d_bc * self.learning_rate\n",
        "\n",
        "        self.wo += d_wo * self.learning_rate\n",
        "        self.bo += d_bo * self.learning_rate\n",
        "\n",
        "        self.wy += d_wy * self.learning_rate\n",
        "        self.by += d_by * self.learning_rate\n",
        "\n",
        "\n",
        "    # Train\n",
        "    def train(self, inputs, labels):\n",
        "        inputs = [oneHotEncode(input) for input in inputs]\n",
        "\n",
        "        for _ in tqdm(range(self.num_epochs)):\n",
        "            predictions = self.forward(inputs)\n",
        "\n",
        "            errors = []\n",
        "            for q in range(len(predictions)):\n",
        "                errors += [-softmax(predictions[q])]\n",
        "                errors[-1][char_to_idx[labels[q]]] += 1\n",
        "\n",
        "            self.backward(errors, self.concat_inputs)\n",
        "\n",
        "    # Test\n",
        "    def test(self, inputs, labels):\n",
        "        accuracy = 0\n",
        "        probabilities = self.forward([oneHotEncode(input) for input in inputs])\n",
        "\n",
        "        output = ''\n",
        "        for q in range(len(labels)):\n",
        "            prediction = idx_to_char[np.random.choice([*range(char_size)], p = softmax(probabilities[q].reshape(-1)))]\n",
        "\n",
        "            output += prediction\n",
        "\n",
        "            if prediction == labels[q]:\n",
        "                accuracy += 1\n",
        "        print(f\"Ground Truth:\\nt{labels}\\n\")\n",
        "        print(f\"Predictions: \\nt{''.join(output)} \\n\")\n",
        "\n",
        "        print(f'Accuracy: {round(accuracy * 100 / len(inputs), 2)}%')\n",
        "\n"
      ],
      "metadata": {
        "id": "ecjK3Ul_hlQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Network\n",
        "hidden_size = 25\n",
        "\n",
        "lstm = LSTM(input_size = char_size + hidden_size, hidden_size = hidden_size, output_size = char_size, num_epochs = 1_000, learning_rate = 0.05)\n",
        "\n",
        "#### Training ####\n",
        "lstm.train(train_X, train_y)\n",
        "\n",
        "#### Testing ####\n",
        "lstm.test(train_X, train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoqIsnEED59A",
        "outputId": "1c20a99f-4351-423c-8db5-6a67eac6b011"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [02:24<00:00,  6.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth:\n",
            "to be, or not to be, that is the question: whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune, or to take arms against a sea of troubles and by opposing end them. to die—to sleep, no more; and by a sleep to say we end the heart-ache and the thousand natural shocks that flesh is heir to: 'tis a consummation devoutly to be wish'd. to die, to sleep; to sleep, perchance to dream—ay, there's the rub: for in that sleep of death what dreams may come, when we have shuffled off this mortal coil, must give us pause—there's the respect that makes calamity of so long life. for who would bear the whips and scorns of time, th'oppressor's wrong, the proud man's contumely, the pangs of dispriz'd love, the law's delay, the insolence of office, and the spurns that patient merit of th'unworthy takes, when he himself might his quietus make\n",
            "\n",
            "Predictions: \n",
            "to be, or not to be, that is the question: whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune, or to take arms against a sea of troubles and by opposing end them. to die—to sleep, no more; and by a sleep to say we end the heart-ache and the thousand natural shocks that flesh is heir to: 'tis a consummation devoutly to be wish'd. to die, to sleep; to sleep, perchance to dream—ay, there's the rub: for in that sleep of death what dreams may come, when we have shuffled off this mortal coil, must give us pause—there's the respect that makes calamity of so long life. for who would bear the whips and scorns of time, th'oppressor's wrong, the proud man's contumely, the pangs of dispriz'd love, the law's delay, the insolence of office, and the spurns that patient merit of th'unworthy takes, when he himself might his quietus make \n",
            "\n",
            "Accuracy: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References**:\n",
        "1.   https://medium.com/mlearning-ai/building-a-neural-network-zoo-from-scratch-the-long-short-term-memory-network-1cec5cf31b7\n"
      ],
      "metadata": {
        "id": "6uTC0XIlN0xi"
      }
    }
  ]
}
